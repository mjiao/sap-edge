# SPDX-FileCopyrightText: 2025 SAP edge team
# SPDX-FileContributor: Manjun Jiao (@mjiao)
#
# SPDX-License-Identifier: Apache-2.0

---
apiVersion: tekton.dev/v1
kind: Task
metadata:
  name: rosa-teardown
spec:
  description: Teardown ROSA cluster and AWS services using Terraform destroy
  params:
    - name: clusterName
      type: string
      description: "ROSA cluster name"
    - name: awsRegion
      type: string
      description: "AWS region for ROSA deployment"
      default: "eu-north-1"
    - name: awsSecretName
      type: string
      description: "Name of the Kubernetes Secret containing AWS credentials"
      default: "aws-credentials-secret"
    - name: redhatTokenSecretName
      type: string
      description: "Name of the Kubernetes Secret containing Red Hat OCM token"
      default: "redhat-token-secret"
    - name: terraformStateS3Bucket
      type: string
      description: "S3 bucket name for Terraform state storage"
      default: "eic-test-rosa-terraform-state"
    - name: terraformStateDynamoDBTable
      type: string
      description: "DynamoDB table for Terraform state locking"
      default: "eic-test-rosa-terraform-state-lock"
  workspaces:
    - name: source
  steps:
    - name: teardown-rosa
      image: registry.access.redhat.com/ubi9/ubi
      timeout: "336h"
      workingDir: $(workspaces.source.path)
      env:
        - name: CLUSTER_NAME
          value: "$(params.clusterName)"
        - name: AWS_REGION
          value: "$(params.awsRegion)"
        - name: AWS_ACCESS_KEY_ID
          valueFrom:
            secretKeyRef:
              name: $(params.awsSecretName)
              key: AWS_ACCESS_KEY_ID
        - name: AWS_SECRET_ACCESS_KEY
          valueFrom:
            secretKeyRef:
              name: $(params.awsSecretName)
              key: AWS_SECRET_ACCESS_KEY
        - name: REDHAT_OCM_TOKEN
          valueFrom:
            secretKeyRef:
              name: $(params.redhatTokenSecretName)
              key: REDHAT_OCM_TOKEN
        - name: TERRAFORM_STATE_S3_BUCKET
          value: "$(params.terraformStateS3Bucket)"
        - name: TERRAFORM_STATE_DYNAMODB_TABLE
          value: "$(params.terraformStateDynamoDBTable)"
      script: |
        #!/usr/bin/env bash
        set -euo pipefail
        
        echo "üóëÔ∏è Starting ROSA cluster and AWS services teardown with Terraform..."
        echo "====================================================================="
        echo "Cluster Name: ${CLUSTER_NAME}"
        echo "AWS Region: ${AWS_REGION}"
        echo ""
        
        # Debug: Check workspace contents
        echo "üîç Current working directory: $(pwd)"
        echo "üìÇ Workspace contents:"
        ls -la
        echo ""
        if [[ -d "rosa" ]]; then
          echo "üìÇ rosa/ directory contents:"
          ls -la rosa/
          echo ""
        fi
        
        # Install required tools
        echo "üì¶ Installing required packages..."
        dnf install -y wget unzip jq tar gzip
        
        # Install Terraform
        echo "üì¶ Installing Terraform..."
        TERRAFORM_VERSION="1.6.6"
        rm -rf terraform terraform_*.zip
        wget -q "https://releases.hashicorp.com/terraform/${TERRAFORM_VERSION}/terraform_${TERRAFORM_VERSION}_linux_amd64.zip"
        unzip -q "terraform_${TERRAFORM_VERSION}_linux_amd64.zip"
        mv terraform /usr/local/bin/
        terraform version
        
        # Install ROSA CLI
        echo "üì¶ Installing ROSA CLI..."
        rm -rf /tmp/rosa-install rosa-linux.tar.gz
        wget -q https://mirror.openshift.com/pub/openshift-v4/clients/rosa/latest/rosa-linux.tar.gz
        mkdir -p /tmp/rosa-install
        tar xzf rosa-linux.tar.gz -C /tmp/rosa-install
        mv /tmp/rosa-install/rosa /usr/local/bin/
        rm -rf /tmp/rosa-install rosa-linux.tar.gz
        rosa version
        
        # Install AWS CLI
        echo "üì¶ Installing AWS CLI..."
        rm -rf aws awscliv2.zip
        curl -s "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
        unzip -q awscliv2.zip
        ./aws/install
        aws --version
        
        # Verify AWS credentials
        echo "üîê Verifying AWS credentials..."
        AWS_IDENTITY=$(aws sts get-caller-identity 2>/dev/null || echo "")
        if [[ -z "${AWS_IDENTITY}" ]]; then
          echo "‚ùå AWS credentials verification failed"
          exit 1
        fi
        AWS_ACCOUNT=$(echo "${AWS_IDENTITY}" | jq -r '.Account')
        AWS_USER=$(echo "${AWS_IDENTITY}" | jq -r '.Arn' | sed 's/.*\///')
        echo "‚úÖ Authenticated as: ${AWS_USER} (Account: ${AWS_ACCOUNT})"
        
        # Login to Red Hat OCM
        echo "üîê Logging into Red Hat OpenShift Cluster Manager..."
        rosa login --token="${REDHAT_OCM_TOKEN}"
        
        # Check if rosa/terraform directory exists
        if [[ ! -d "rosa/terraform" ]]; then
          echo "‚ùå rosa/terraform directory not found in workspace"
          echo "‚ö†Ô∏è  Falling back to ROSA CLI cleanup only..."
          
          # Fallback to ROSA CLI cleanup
          if rosa describe cluster -c "${CLUSTER_NAME}" &>/dev/null; then
            echo "üóëÔ∏è Deleting cluster with ROSA CLI..."
            rosa delete cluster -c "${CLUSTER_NAME}" --yes || true
            echo "‚ö†Ô∏è  Note: Terraform-managed AWS resources (PostgreSQL, Redis, S3) NOT deleted"
            echo "   Manual cleanup required: ./hack/rosa/cleanup-rosa-resources.sh"
          else
            echo "‚úÖ No cluster found"
          fi
          exit 0
        fi
        
        # Navigate to Terraform directory
        cd rosa/terraform
        
        # Compute cluster-specific S3 key
        TERRAFORM_BACKEND_S3_KEY="rosa/${CLUSTER_NAME}/terraform.tfstate"
        
        # Initialize Terraform with S3 backend
        echo "üîß Initializing Terraform with S3 backend..."
        echo "   Bucket: ${TERRAFORM_STATE_S3_BUCKET}"
        echo "   Key: ${TERRAFORM_BACKEND_S3_KEY}"
        echo "   Region: ${AWS_REGION}"
        
        if ! terraform init \
          -backend-config="bucket=${TERRAFORM_STATE_S3_BUCKET}" \
          -backend-config="key=${TERRAFORM_BACKEND_S3_KEY}" \
          -backend-config="region=${AWS_REGION}" \
          -backend-config="dynamodb_table=${TERRAFORM_STATE_DYNAMODB_TABLE}" \
          -backend-config="encrypt=true"; then
          echo "‚ùå Terraform init failed"
          echo "‚ö†Ô∏è  This might mean no Terraform state exists for this cluster"
          echo "üîç Checking if cluster exists via ROSA CLI..."
          
          cd ../..
          if rosa describe cluster -c "${CLUSTER_NAME}" &>/dev/null; then
            echo "‚ö†Ô∏è  Cluster exists but no Terraform state found"
            echo "üóëÔ∏è Deleting cluster with ROSA CLI..."
            rosa delete cluster -c "${CLUSTER_NAME}" --yes || true
            echo "‚ö†Ô∏è  Note: Manual cleanup of AWS resources may be required"
            echo "   Use: ./hack/rosa/cleanup-rosa-resources.sh"
          else
            echo "‚úÖ No cluster found, nothing to delete"
          fi
          exit 0
        fi
        
        # List current resources
        echo ""
        echo "üìã Current Terraform-managed resources:"
        terraform state list || echo "No resources found"
        echo ""
        
        # Show what will be destroyed
        echo "üîç Planning destruction..."
        terraform plan -destroy -var="redhat_ocm_token=${REDHAT_OCM_TOKEN}" || true
        
        # Staged destroy to avoid VPC dependency issues
        # ROSA creates security groups and ENIs in the VPC that are cleaned up asynchronously.
        # We destroy ROSA first, wait for AWS cleanup, then destroy the rest.
        echo ""
        echo "üóëÔ∏è Destroying Terraform-managed resources (staged approach)..."
        echo "‚ö†Ô∏è  This will delete:"
        echo "  - ROSA cluster and all workloads"
        echo "  - PostgreSQL RDS database"
        echo "  - Redis ElastiCache cluster"
        echo "  - S3 buckets (Quay registry)"
        echo "  - IAM users and roles"
        echo "  - VPC and networking"
        echo "  - Security groups"
        echo ""

        # Stage 1: Destroy ROSA cluster first
        echo "üîÑ Stage 1: Destroying ROSA cluster..."
        if terraform destroy -auto-approve -var="redhat_ocm_token=${REDHAT_OCM_TOKEN}" -target=module.rosa-hcp; then
          echo "‚úÖ ROSA cluster destroyed"
        else
          echo "‚ö†Ô∏è  ROSA cluster destruction had issues, continuing..."
        fi

        # Stage 2: Actively clean up ROSA-created VPC resources (security groups, ENIs)
        # ROSA creates these outside of Terraform, so they block VPC deletion.
        echo ""
        echo "üßπ Stage 2: Cleaning up ROSA-created resources in VPC..."

        # Get VPC ID from Terraform state
        VPC_ID=$(terraform state show 'module.vpc[0].aws_vpc.this[0]' 2>/dev/null \
          | grep '^\s*id\s' | head -1 | awk '{print $NF}' | tr -d '"') || true

        if [[ -n "${VPC_ID}" ]]; then
          echo "   VPC: ${VPC_ID}"

          # Brief wait for initial async cleanup
          echo "   Waiting 60 seconds for initial AWS cleanup..."
          sleep 60

          # Delete ENIs (network interfaces) created by ROSA
          echo "   Deleting network interfaces..."
          ENI_IDS=$(aws ec2 describe-network-interfaces \
            --region "${AWS_REGION}" \
            --filters "Name=vpc-id,Values=${VPC_ID}" \
            --query "NetworkInterfaces[].NetworkInterfaceId" \
            --output text 2>/dev/null) || true

          for ENI_ID in ${ENI_IDS}; do
            echo "     Detaching/deleting ENI: ${ENI_ID}"
            ATTACHMENT_ID=$(aws ec2 describe-network-interfaces \
              --region "${AWS_REGION}" \
              --network-interface-ids "${ENI_ID}" \
              --query "NetworkInterfaces[0].Attachment.AttachmentId" \
              --output text 2>/dev/null) || true
            if [[ -n "${ATTACHMENT_ID}" && "${ATTACHMENT_ID}" != "None" ]]; then
              aws ec2 detach-network-interface --region "${AWS_REGION}" \
                --attachment-id "${ATTACHMENT_ID}" --force 2>/dev/null || true
              sleep 5
            fi
            aws ec2 delete-network-interface --region "${AWS_REGION}" \
              --network-interface-id "${ENI_ID}" 2>/dev/null || true
          done

          # Delete non-default security groups in the VPC
          echo "   Deleting security groups..."
          SG_IDS=$(aws ec2 describe-security-groups \
            --region "${AWS_REGION}" \
            --filters "Name=vpc-id,Values=${VPC_ID}" \
            --query "SecurityGroups[?GroupName!='default'].GroupId" \
            --output text 2>/dev/null) || true

          # First pass: remove all ingress/egress rules (clears cross-SG references)
          for SG_ID in ${SG_IDS}; do
            INGRESS_RULES=$(aws ec2 describe-security-group-rules \
              --region "${AWS_REGION}" \
              --filters "Name=group-id,Values=${SG_ID}" \
              --query "SecurityGroupRules[?!IsEgress].SecurityGroupRuleId" \
              --output text 2>/dev/null) || true
            if [[ -n "${INGRESS_RULES}" && "${INGRESS_RULES}" != "None" ]]; then
              aws ec2 revoke-security-group-ingress --region "${AWS_REGION}" \
                --group-id "${SG_ID}" --security-group-rule-ids ${INGRESS_RULES} 2>/dev/null || true
            fi
            EGRESS_RULES=$(aws ec2 describe-security-group-rules \
              --region "${AWS_REGION}" \
              --filters "Name=group-id,Values=${SG_ID}" \
              --query "SecurityGroupRules[?IsEgress].SecurityGroupRuleId" \
              --output text 2>/dev/null) || true
            if [[ -n "${EGRESS_RULES}" && "${EGRESS_RULES}" != "None" ]]; then
              aws ec2 revoke-security-group-egress --region "${AWS_REGION}" \
                --group-id "${SG_ID}" --security-group-rule-ids ${EGRESS_RULES} 2>/dev/null || true
            fi
          done
          # Second pass: delete the security groups
          for SG_ID in ${SG_IDS}; do
            echo "     Deleting security group: ${SG_ID}"
            aws ec2 delete-security-group --region "${AWS_REGION}" \
              --group-id "${SG_ID}" 2>/dev/null || true
          done

          # Delete any remaining load balancers in the VPC
          echo "   Checking for load balancers..."
          LB_ARNS=$(aws elbv2 describe-load-balancers \
            --region "${AWS_REGION}" \
            --query "LoadBalancers[?VpcId=='${VPC_ID}'].LoadBalancerArn" \
            --output text 2>/dev/null) || true
          for LB_ARN in ${LB_ARNS}; do
            echo "     Deleting load balancer: ${LB_ARN}"
            aws elbv2 delete-load-balancer --region "${AWS_REGION}" \
              --load-balancer-arn "${LB_ARN}" 2>/dev/null || true
          done
          # Also check classic LBs
          CLB_NAMES=$(aws elb describe-load-balancers \
            --region "${AWS_REGION}" \
            --query "LoadBalancerDescriptions[?VPCId=='${VPC_ID}'].LoadBalancerName" \
            --output text 2>/dev/null) || true
          for CLB_NAME in ${CLB_NAMES}; do
            echo "     Deleting classic LB: ${CLB_NAME}"
            aws elb delete-load-balancer --region "${AWS_REGION}" \
              --load-balancer-name "${CLB_NAME}" 2>/dev/null || true
          done

          if [[ -n "${LB_ARNS}" || -n "${CLB_NAMES}" ]]; then
            echo "   Waiting 30 seconds for LB cleanup..."
            sleep 30
          fi

          echo "   ‚úÖ VPC resource cleanup done"
        else
          echo "   ‚ö†Ô∏è  Could not determine VPC ID, waiting 5 minutes instead..."
          sleep 300
        fi

        # Stage 3: Destroy remaining infrastructure (VPC, S3, etc.) with retries
        echo ""
        echo "üîÑ Stage 3: Destroying remaining infrastructure..."
        DESTROY_SUCCESS=false
        for ATTEMPT in 1 2 3; do
          echo "   Attempt ${ATTEMPT}/3..."
          if terraform destroy -auto-approve -var="redhat_ocm_token=${REDHAT_OCM_TOKEN}"; then
            echo "‚úÖ Terraform destroy completed successfully"
            echo "‚úÖ All resources cleaned up!"
            DESTROY_SUCCESS=true
            break
          else
            echo "   ‚ö†Ô∏è  Attempt ${ATTEMPT} failed"
            if [[ ${ATTEMPT} -lt 3 ]]; then
              echo "   Waiting 60 seconds before retry..."
              sleep 60
            fi
          fi
        done

        if [[ "${DESTROY_SUCCESS}" != "true" ]]; then
          echo "‚ùå Terraform destroy failed after 3 attempts"
          echo ""
          echo "üìã Remaining resources:"
          terraform state list || echo "Unable to list state"
          echo ""
          echo "üí° Troubleshooting steps:"
          echo "   1. Check AWS console for remaining resources"
          echo "   2. Run manual cleanup: ./hack/rosa/cleanup-rosa-resources.sh"
          echo "   3. Check ROSA cluster: rosa describe cluster -c ${CLUSTER_NAME}"
          exit 1
        fi
        
        cd ../..
        
        # Clean up workspace files
        echo "üßπ Cleaning up workspace files..."
        rm -f rosa-deployment-info.txt
        rm -f cluster-info.json
        rm -f admin-creds.json
        rm -f kubeconfig
        
        echo ""
        echo "‚úÖ ROSA teardown completed!"
        echo "üìù Note: It may take a few minutes for all AWS resources to be fully deleted"

